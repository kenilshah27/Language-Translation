{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to French Text Seq2Seq using Attention model ",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNG3pgYe_Qzm",
        "colab_type": "text"
      },
      "source": [
        "The dataset is taken from the site: http://www.manythings.org/anki/\n",
        "\n",
        "There are many datasets which could have been taken. I selected English to French as it had enough samples to train the model effectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnvFtHXv-2RX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c93e2918-ef4a-45bd-a043-43aa903fd59a"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,Concatenate\n",
        "from keras.layers import RepeatVector, Concatenate, Activation, Dot, Lambda          # New Layers\n",
        "import keras.backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMWYaeG_-8YR",
        "colab_type": "code",
        "outputId": "5ac79640-b106-4632-8297-6f770054ab0b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33fc73e1-a734-4575-aa63-38d768c47a36\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-33fc73e1-a734-4575-aa63-38d768c47a36\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fra.txt to fra.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4gSRjYntqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Softmax function to calculate alpha values for the context in attention\n",
        "\n",
        "# Expected shape is  (batch_size,TimeStep,Dimension)\n",
        "\n",
        "def softmax_over_time(x):\n",
        "  assert (K.ndim(x) > 2)\n",
        "  e = K.exp(x - K.max(x,axis = 1, keepdims = True))\n",
        "  s = K.sum(e,axis=1,keepdims = True)\n",
        "  return e/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY4NaVN5_v-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_table('/content/fra.txt',header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9PKMOVyAeBj",
        "colab_type": "code",
        "outputId": "7f844da4-d283-4a2c-9cc7-2f243ca7d8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Dataset for the model\n",
        "\n",
        "df.columns = ['English','French']\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English    French\n",
              "0     Go.      Va !\n",
              "1     Hi.   Salut !\n",
              "2     Hi.    Salut.\n",
              "3    Run!   Cours !\n",
              "4    Run!  Courez !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNwUcZrABNaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting Parameters\n",
        "\n",
        "BATCH_SIZE = 32            # Batch size for the training set. After each BATCH_SIZE the weights will be updated\n",
        "EPOCHS = 10                # Number of times we will train the model\n",
        "LSTM_units = 512           # Output nits for the LSTM Layer\n",
        "MAX_SEQUENCE_LENGTH = 100  # Maximum number of words in a single sentence\n",
        "VOCAB_SIZE = 20000         # Vocab size for the dataset\n",
        "EMBEDDING_DIM = 50         # Embedding units to represent a single word in English\n",
        "EMBEDDING_DIM_FRENCH = 100 # Embedding units to represent a single word in French"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk2IVfhjCQIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_text_input = df['English'][:15000].values                                 # Input for the training Encoder\n",
        "french_text_input = df['French'][:15000].apply(lambda x:'<sos> ' + x).values      # Input for the training Decoder\n",
        "french_text_output = df['French'][:15000].apply(lambda x:x + ' <eos>').values     # Output for the training Decoder\n",
        "\n",
        "# We are using <sos> and <eos> as we will be using Teacher Forcing."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4pXHk2C-_2",
        "colab_type": "code",
        "outputId": "9f7ad6aa-11ec-41d3-9de5-7e2667083eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of samples for the training data: \",len(english_text_input))  # 170190 samples"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples for the training data:  15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gGR3cAqDAT2",
        "colab_type": "code",
        "outputId": "7a9f003c-9985-49fc-9913-ecc9dce853dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tokenize the English Text\n",
        "tokenizer_english = Tokenizer(num_words=VOCAB_SIZE)\n",
        "sentences = tokenizer_english.fit_on_texts(english_text_input)\n",
        "english_sequences_input = tokenizer_english.texts_to_sequences(english_text_input)\n",
        "english_sequences_input[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[18], [668], [668], [146], [146]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088VejLCF7D2",
        "colab_type": "text"
      },
      "source": [
        "The words are now converted into numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjcSsIkVGJSI",
        "colab_type": "code",
        "outputId": "0f9339cc-5a75-4f3e-e268-3cb80e2da399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get the word to index Mapping\n",
        "word2idx_english = tokenizer_english.word_index\n",
        "print('Unique english words: ',len(word2idx_english))    # Identified 14384 unique letters"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique english words:  2921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av5sRkyzDT22",
        "colab_type": "code",
        "outputId": "20d7b236-2495-41f0-fe08-f467ca5b8723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Tokenize the French Text\n",
        "tokenizer_french = Tokenizer(num_words=VOCAB_SIZE,filters = '')\n",
        "sentences = tokenizer_french.fit_on_texts(french_text_input + french_text_output)  # We use both the dataset so that <sos> and <eos> are also included in the tokenize set of words\n",
        "french_sequences_input = tokenizer_french.texts_to_sequences(french_text_input)\n",
        "french_sequences_output = tokenizer_french.texts_to_sequences(french_text_output)\n",
        "print('Input: ',french_sequences_input[:5])\n",
        "print('Output: ',french_sequences_output[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [[1, 58, 6], [1, 1208, 6], [1], [1], [1]]\n",
            "Output:  [[58, 6, 2], [1208, 6, 2], [2], [2], [2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6DX_L2zHDaY",
        "colab_type": "code",
        "outputId": "da5ea147-2d14-474a-9b50-7577cf28f283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get the word to index Mapping\n",
        "\n",
        "word2idx_french = tokenizer_french.word_index\n",
        "print('Unique french words: ',len(word2idx_french))    # Identified 14384 unique letters\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique french words:  16547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNZAqGn0HKo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting Max length for both the texts\n",
        "max_english = max(len(s) for s in english_sequences_input)   # Max sequence length in English\n",
        "max_french = max(len(s) for s in french_sequences_input)     # Max sequence length in French\n",
        "num_words_french = len(word2idx_french) + 1                  # Possible outputs for the french language"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBtOwn-HVsf",
        "colab_type": "code",
        "outputId": "afafd362-0bf8-4f95-e1a5-17dc32dc9cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Padding the sequences\n",
        "\n",
        "encode_english_input = pad_sequences(english_sequences_input,maxlen = max_english,padding = 'post')\n",
        "decode_french_input = pad_sequences(french_sequences_input,maxlen = max_french,padding = 'post')\n",
        "decode_french_output = pad_sequences(french_sequences_output,maxlen = max_french,padding = 'post')\n",
        "\n",
        "# Size of the input and output\n",
        "\n",
        "print('Size of Encode Input: ',encode_english_input.shape)\n",
        "print('Size of Decode Input: ',decode_french_input.shape)\n",
        "print('Size of Decode Output: ',decode_french_output.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Encode Input:  (15000, 5)\n",
            "Size of Decode Input:  (15000, 12)\n",
            "Size of Decode Output:  (15000, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9dXc4YKMzG",
        "colab_type": "code",
        "outputId": "9c88ea1b-eabc-41eb-a599-b08d1edde7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading pretrained word vector\n",
        "\n",
        "word2vec = {}\n",
        "with open(os.path.join('/content/drive/My Drive/Colab Notebooks/Dataset/glove.6B.50d.txt')) as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MjbWINzKpwt",
        "colab_type": "code",
        "outputId": "8e4db1d7-de53-48ef-dd84-c60fa6712f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_words = min(VOCAB_SIZE, len(word2idx_english) + 1)\n",
        "\n",
        "print('Number of words',num_words)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # Creating the embedding matrix with each word having dimension of 50\n",
        "print('Shape of Embedding Matrix',embedding_matrix.shape)\n",
        "\n",
        "for word, i in word2idx_english.items():\n",
        "  if i < VOCAB_SIZE:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words 2922\n",
            "Shape of Embedding Matrix (2922, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRcLBZsLPVH",
        "colab_type": "code",
        "outputId": "2afdcfdc-a019-4194-dd50-6b61e3376d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [ 1.18910000e-01,  1.52549997e-01, -8.20730031e-02, ...,\n",
              "        -5.75119972e-01, -2.66710013e-01,  9.21209991e-01],\n",
              "       [-1.09190005e-03,  3.33240002e-01,  3.57430011e-01, ...,\n",
              "        -4.56970006e-01, -4.89690006e-02,  1.13160002e+00],\n",
              "       ...,\n",
              "       [ 2.15690002e-01, -9.00229990e-01,  6.82510018e-01, ...,\n",
              "         4.65460002e-01,  1.81079999e-01, -1.22239999e-01],\n",
              "       [ 9.63559985e-01, -5.39669991e-01,  2.77429998e-01, ...,\n",
              "        -3.87650013e-01,  1.31150007e-01,  6.29419982e-01],\n",
              "       [-4.29910004e-01,  5.82780004e-01, -8.21919963e-02, ...,\n",
              "        -6.38769984e-01, -6.83719963e-02, -8.71749997e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UddAM0nOs-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWdD-GHVLcEe",
        "colab_type": "text"
      },
      "source": [
        "# MODEL CREATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpW-lu6oLfGu",
        "colab_type": "code",
        "outputId": "4692581a-5667-4253-eaeb-f59cc62d4143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Embedding Layer\n",
        "\n",
        "print('Number of words ',num_words)\n",
        "\n",
        "embedding_layer = Embedding(input_dim = num_words, output_dim = EMBEDDING_DIM, weights = [embedding_matrix],input_length = max_english)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 15:48:41.614988 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of words  2922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkaGbsGPMJxM",
        "colab_type": "code",
        "outputId": "5ca2f009-9904-488c-d065-d63b99607434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating the target variable\n",
        "\n",
        "decoder_target_onehot = np.zeros((len(english_text_input),max_french,num_words_french),dtype = 'float32')\n",
        "print('Shape: ',decoder_target_onehot.shape)   \n",
        "\n",
        "# 10000 represents each sequence\n",
        "# 11 represents max length in english\n",
        "# 11903 represents max length in french"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  (15000, 12, 16548)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PhTdCQfMuAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assiging the values to the output\n",
        "\n",
        "for i,d in enumerate(decode_french_output):\n",
        "  for t,word in enumerate(d):\n",
        "    decoder_target_onehot[i,t,word] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLEzNK2DQfPf",
        "colab_type": "text"
      },
      "source": [
        "## Adding Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34phGpjK3tKl",
        "colab_type": "code",
        "outputId": "417fc46f-4495-49d3-f2b9-77265366f33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "## Setting the encoder\n",
        "## Embedding Layer\n",
        "\n",
        "encoder_input = Input(shape = (max_english,))\n",
        "\n",
        "print('Encoder input size before embedding',encoder_input.shape)\n",
        "\n",
        "x = embedding_layer(encoder_input)\n",
        "\n",
        "print('Encoder input size after embedding',x.shape)    # So each of the english words are now represented by 50 wordvector \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 15:48:51.164558 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0807 15:48:51.177212 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0807 15:48:51.189575 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0807 15:48:51.190344 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoder input size before embedding (?, 5)\n",
            "Encoder input size after embedding (?, 5, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V757qzVBwq_1",
        "colab_type": "code",
        "outputId": "3b9aba77-6dd1-4686-97f8-b5e115cc68c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "## Setting the encoder\n",
        "## Bidirectional LSTM Layer\n",
        "\n",
        "encoder_lstm = Bidirectional(LSTM(LSTM_units,return_sequences = True,dropout = 0.2))\n",
        "encoder_lstm_2 = LSTM(LSTM_units*2,return_sequences = True,dropout = 0.2)\n",
        "\n",
        "print('Input size before LSTM Layer',x.shape)\n",
        "\n",
        "encoder_outputs = encoder_lstm(x)      # Would have all the hidden states present\n",
        "encoder_outputs = encoder_lstm_2(encoder_outputs) \n",
        "\n",
        "print('Input size after LSTM Layer',encoder_outputs.shape)    # So each of the 50 wordvector is now represented by size of 512\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input size before LSTM Layer (?, 5, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0807 15:48:56.633262 140107071637376 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input size after LSTM Layer (?, ?, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "samfml3MxS_7",
        "colab_type": "code",
        "outputId": "1cc044aa-0f73-4923-e45a-982a24fc0f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Setting the decoder\n",
        "## Embedding Layer\n",
        "\n",
        "decoder_input = Input(shape = (max_french,))\n",
        "\n",
        "print('Decoder input size before embedding',decoder_input.shape)\n",
        "\n",
        "decoder_embedding = Embedding(num_words_french,EMBEDDING_DIM_FRENCH)\n",
        "decoder_x = decoder_embedding(decoder_input)\n",
        "\n",
        "print('Decoder input size after embedding',decoder_x.shape)    # So each of the english words are now represented by 100 wordvector \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder input size before embedding (?, 12)\n",
            "Decoder input size after embedding (?, 12, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHi7lv7xzKHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ATTENTION ###\n",
        "\n",
        "attention_repeat_layer = RepeatVector(max_english)   # Repeat a vector max_english times\n",
        "attention_concatenate_layer = Concatenate(axis = -1) # Concatenate previous state with each hidden layer\n",
        "\n",
        "attention_dense_layer_1 = Dense(1024,activation = 'tanh')  # A dense layer for the input [previous_state,hidden_layer]\n",
        "# attention_dense_layer_2 = Dense(1024,activation = 'tanh')\n",
        "# attention_dense_layer_3 = Dense(1024,activation = 'tanh')\n",
        "# attention_dense_layer_4 = Dense(1024,activation = 'tanh')\n",
        "# attention_dense_layer_5 = Dense(512,activation = 'tanh')\n",
        "attention_dense_layer_6 = Dense(1,activation = softmax_over_time)\n",
        "\n",
        "\n",
        "attention_dot = Dot(axes = 1)  # Dot product for alpha and the hidden layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOtsG_Xu0Xzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention_step(hidden_state,previous_state):\n",
        "  \n",
        "  # previous_state is of size LSTM_units\n",
        "  \n",
        "  previous_state = attention_repeat_layer(previous_state)\n",
        "  # previous_state will be of size (Tx,LSTM_units) now\n",
        "  \n",
        "  # hidden state is of size (Tx,LSTM_units*2)\n",
        "  \n",
        "  x = attention_concatenate_layer([hidden_state,previous_state])\n",
        "  # x will be of size (Tx,LSTM_units*2(from hidden state) + LSTM_units(from previous state))\n",
        "  \n",
        "  x = attention_dense_layer_1(x)\n",
        "  # x will be of size (Tx,10)\n",
        "  \n",
        "#   x = attention_dense_layer_2(x)\n",
        "  \n",
        "#   x = attention_dense_layer_3(x)\n",
        "  \n",
        "#   x = attention_dense_layer_4(x)\n",
        "  \n",
        "#   x = attention_dense_layer_5(x)\n",
        "  \n",
        "  alpha = attention_dense_layer_6(x)\n",
        "  # alpha will be of size (Tx,1)\n",
        "  \n",
        "  \n",
        "  context = attention_dot([alpha,hidden_state])    \n",
        "  \n",
        "  \n",
        "  # shape of context will be (1,LSTM_units*2)\n",
        "  \n",
        "  \"\"\"\n",
        "  Size of shape: (?, ?, LSTM_units*2(shape from encoder_outputs))\n",
        "  Previous state shape before RepeatVector function:  (?, LSTM_units(shape from initial_s))\n",
        "  Previous state shape after RepeatVector function:  (?, 5(Tx), 256)\n",
        "  Shape after concatenation:  (?, 5(Tx), 768(LSTM_units*2+LSTM_units))\n",
        "  Shape after 1 Dense Layer:  (?, 5(Tx), 10(from dense layer 1))\n",
        "  Shape of alpha:  (?, 5(Tx), 1)\n",
        "  Shape of context:  (?, 1, 512(LSTM_units*2: Explained below))\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  WORKING OF DOT PRODUCT\n",
        "  \n",
        "  @ Assuming Tx is 5 and LSTM_units is 2 so \n",
        "  \n",
        "  alpha = [\n",
        "          1\n",
        "          2\n",
        "          3\n",
        "          4\n",
        "          5\n",
        "          ]\n",
        "  \n",
        "  hidden_state = [\n",
        "          1 2 3 4\n",
        "          1 2 3 4\n",
        "          1 2 3 4\n",
        "          1 2 3 4\n",
        "          1 2 3 4\n",
        "  ] \n",
        "  \n",
        "  Now (alpha).(hidden_state) \n",
        "   \n",
        "          [       [            \n",
        "          1       1 2 3 4      [1*1 + 2*1 + 3*1 + 4*1 + 5*1,1*2 + 2*2 + 3*2 + 4*2 + 5*2,1*3 + 2*3 + 3*3 + 4*3 + 5*3,1*4 + 2*4 + 3*4 + 4*4 + 5*4] = [15.30,45,60]\n",
        "          2       1 2 3 4      \n",
        "          3    .  1 2 3 4   =  \n",
        "          4       1 2 3 4      \n",
        "          5       1 2 3 4      \n",
        "          ]             ]\n",
        "  \n",
        "  Hence the shape of context will be (1,LSTM_units*2) = (1,4)\n",
        "  \n",
        "  \"\"\"\n",
        "  return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOuVY1ap1q7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_lstm = LSTM(LSTM_units,return_state = True)\n",
        "decoder_dense_1 = Dense(LSTM_units*2,activation = 'relu')\n",
        "# decoder_dense_2 = Dense(LSTM_units*3,activation = 'relu')\n",
        "# decoder_dense_3 = Dense(LSTM_units*4,activation = 'relu')\n",
        "# decoder_dense_4 = Dense(LSTM_units*5,activation = 'relu')\n",
        "# decoder_dense_5 = Dense(LSTM_units*5,activation = 'relu')\n",
        "\n",
        "decoder_dense = Dense(num_words_french,activation = 'softmax')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJKoB_Gx2WSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_s = Input(shape = (LSTM_units,))       # Initial hidden state to be provided to the decoder LSTM\n",
        "initial_c = Input(shape = (LSTM_units,))       # Initial cell state to be provided to the decoder LSTM\n",
        "\n",
        "context_last_word_concat_layer = Concatenate(axis = 2) # Concatenate the context vector and the previous output generated to provide as input\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CU4qdHi3AxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = initial_s\n",
        "c = initial_c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm-M6xs-3Ca5",
        "colab_type": "code",
        "outputId": "6644bc3d-261f-4cd4-d6a3-01060a079288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "outputs = [] # To collect output for each single input sentence\n",
        "\n",
        "for t in range(max_french):        # So that we collect upto Ty(max output length) values for each input statement\n",
        "  \n",
        "  context = attention_step(encoder_outputs,s)\n",
        "  # Get the context with shape (1,LSTM_units*2)\n",
        "  \n",
        "  selector = Lambda(lambda x: x[:,t:t+1])    # Correct input to provide for teacher forcing\n",
        "  xt = selector(decoder_x)                   # Selecting the correct input\n",
        "  \n",
        "  # Shape of xt is (1,EMBEDDING_DIM_FRENCH)\n",
        "  \n",
        "  decoder_lstm_input = context_last_word_concat_layer([context,xt])\n",
        "  \n",
        "  print(\"Shape of decoder input (concatenate of context and input):\",decoder_lstm_input.shape)\n",
        "  # Shape of xt is (1,LSTM_units*2(shape of context)+EMBEDDING_DIM_FRENCH)\n",
        "  \n",
        "  o,s,c = decoder_lstm(decoder_lstm_input,initial_state = [s,c])       # We get new s and c \n",
        "  \n",
        "  print(\"Shape of decoder output:\",o.shape)   #(LSTM_units)\n",
        "  \n",
        "  decoder_outputs = decoder_dense_1(o)\n",
        "#   decoder_outputs = decoder_dense_2(decoder_outputs)\n",
        "#   decoder_outputs = decoder_dense_3(decoder_outputs)\n",
        "#   decoder_outputs = decoder_dense_4(decoder_outputs)   # now decoder_outputs is the probability\n",
        "#   decoder_outputs = decoder_dense_5(decoder_outputs)\n",
        "  \n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  \n",
        "  print(\"Shape of decoder output(after dense):\",decoder_outputs.shape) #(max_french)\n",
        "  \n",
        "  outputs.append(decoder_outputs) \n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n",
            "Shape of decoder input (concatenate of context and input): (?, 1, 1124)\n",
            "Shape of decoder output: (?, 512)\n",
            "Shape of decoder output(after dense): (?, 16548)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEgncGF8FjU4",
        "colab_type": "code",
        "outputId": "3ab15504-e5b6-442f-d4f3-a0710e24b6b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "outputs # OUTPUT IS OF SIZE Ty,?,16548 but we do not need that we need to have Ty as the second dimension so we will do a stack and transpose function"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'dense_4/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_1/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_2/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_3/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_4/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_5/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_6/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_7/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_8/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_9/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_10/Softmax:0' shape=(?, 16548) dtype=float32>,\n",
              " <tf.Tensor 'dense_4_11/Softmax:0' shape=(?, 16548) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjGUnXa0R5rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stack_and_transpose(x):\n",
        "  return K.permute_dimensions(K.stack(x),pattern = (1,0,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K4XifM1SqMZ",
        "colab_type": "code",
        "outputId": "2f4cb870-67a7-41f2-f9d2-a7d0a05f8420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stacker = Lambda(stack_and_transpose)\n",
        "outputs = stacker(outputs)\n",
        "outputs                        # Appropriate shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'lambda_13/transpose:0' shape=(?, 12, 16548) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVJdGTGCTMix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model([encoder_input,decoder_input,initial_s,initial_c],outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fysfUEeT-mR",
        "colab_type": "code",
        "outputId": "d5a1e63b-266e-4c7f-b585-c9eb027b45b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy',metrics = ['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 15:49:05.231151 140107071637376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLpNP1PUVOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = np.zeros((15000,LSTM_units))  # To pass as initial s and c for the decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLg0-ZYkUmG5",
        "colab_type": "code",
        "outputId": "51b59e55-b17c-4650-9133-15332a9d96c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "r = model.fit([encode_english_input,decode_french_input,z,z],decoder_target_onehot,batch_size = BATCH_SIZE,epochs = EPOCHS,validation_split = 0.1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 15:49:05.611253 140107071637376 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 13500 samples, validate on 1500 samples\n",
            "Epoch 1/10\n",
            "13500/13500 [==============================] - 100s 7ms/step - loss: 1.9624 - acc: 0.7215 - val_loss: 1.9532 - val_acc: 0.7138\n",
            "Epoch 2/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 1.4609 - acc: 0.7799 - val_loss: 1.7814 - val_acc: 0.7345\n",
            "Epoch 3/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 1.2884 - acc: 0.8003 - val_loss: 1.7026 - val_acc: 0.7494\n",
            "Epoch 4/10\n",
            "13500/13500 [==============================] - 79s 6ms/step - loss: 1.1796 - acc: 0.8119 - val_loss: 1.6587 - val_acc: 0.7530\n",
            "Epoch 5/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 1.0957 - acc: 0.8201 - val_loss: 1.6168 - val_acc: 0.7608\n",
            "Epoch 6/10\n",
            "13500/13500 [==============================] - 81s 6ms/step - loss: 1.0135 - acc: 0.8283 - val_loss: 1.5831 - val_acc: 0.7664\n",
            "Epoch 7/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 0.9428 - acc: 0.8357 - val_loss: 1.5997 - val_acc: 0.7571\n",
            "Epoch 8/10\n",
            "13500/13500 [==============================] - 78s 6ms/step - loss: 0.8755 - acc: 0.8424 - val_loss: 1.6152 - val_acc: 0.7626\n",
            "Epoch 9/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 0.8111 - acc: 0.8510 - val_loss: 1.6198 - val_acc: 0.7618\n",
            "Epoch 10/10\n",
            "13500/13500 [==============================] - 80s 6ms/step - loss: 0.7518 - acc: 0.8577 - val_loss: 1.6284 - val_acc: 0.7668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJVUQ1i4VJih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_input,encoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfVbItiQWvnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "91739235-4e32-4b63-95c8-b4f3a9b69d87"
      },
      "source": [
        "encoder_outputs_as_input = Input(shape = (max_english,LSTM_units*2,))\n",
        "decoder_single_input = Input(shape = (1,))\n",
        "decoder_single_input_emb = decoder_embedding(decoder_single_input)\n",
        "\n",
        "print(\"Shape of the encoder input for the testing decoder: \",encoder_outputs_as_input.shape)\n",
        "print(\"Shape of the decoder input for the testing decoder before embedding: \",decoder_single_input.shape)\n",
        "print(\"Shape of the decoder input for the testing decoder after embedding: \",decoder_single_input_emb.shape)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the encoder input for the testing decoder:  (?, 5, 1024)\n",
            "Shape of the decoder input for the testing decoder before embedding:  (?, 1)\n",
            "Shape of the decoder input for the testing decoder after embedding:  (?, 1, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46JATInZXdCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3546a1b-de0a-4455-bf7a-ca1a4df2c1d5"
      },
      "source": [
        "context = attention_step(encoder_outputs_as_input,initial_s)\n",
        "\n",
        "print('Shape of context: ',context.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of context:  (?, 1, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i7M9_FSX7xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c88618e-b917-44e9-ce6c-5e9d8933068d"
      },
      "source": [
        "decoder_lstm_input = context_last_word_concat_layer([context,decoder_single_input_emb])\n",
        "\n",
        "print('Shape of decoder lstm input for testing: ',decoder_lstm_input.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of decoder lstm input for testing:  (?, 1, 1124)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQO04mfKZVZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c059ade9-6bff-4271-81ff-b07615113ff0"
      },
      "source": [
        "o,s,c = decoder_lstm(decoder_lstm_input,initial_state = [initial_s,initial_c])\n",
        "\n",
        "decoder_outputs = decoder_dense_1(o)\n",
        "# decoder_outputs = decoder_dense_2(decoder_outputs)\n",
        "# decoder_outputs = decoder_dense_3(decoder_outputs)\n",
        "# decoder_outputs = decoder_dense_4(decoder_outputs)   # now decoder_outputs is the probability\n",
        "# decoder_outputs = decoder_dense_5(decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "print('Decoder output:',decoder_outputs.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output: (?, 16548)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8urfFiZw6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_model = Model([decoder_single_input,encoder_outputs_as_input,initial_s,initial_c],[decoder_outputs,s,c])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrRBWTmWa3Dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2word_english = {v:k for k,v in word2idx_english.items()}\n",
        "idx2word_french = {v:k for k,v in word2idx_french.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZnMfy2RbEIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sentences(input_sequence):\n",
        "  \n",
        "  encoder_outputs = encoder_model.predict(input_sequence)\n",
        "  \n",
        "  \n",
        "  target = np.zeros((1,1))\n",
        "  \n",
        "  target[0,0] = word2idx_french['<sos>']\n",
        "  \n",
        "  \n",
        "  eos = word2idx_french['<eos>']\n",
        "  \n",
        "  s = np.zeros((1,LSTM_units))\n",
        "  c = np.zeros((1,LSTM_units))\n",
        "  \n",
        "  output_sequence = []\n",
        "  \n",
        "  for _ in range(max_french):\n",
        "    \n",
        "    o,s,c = decoder_model.predict([target,encoder_outputs,s,c])\n",
        "\n",
        "    idx = np.argmax(o.flatten())\n",
        "    \n",
        "    if eos == idx:\n",
        "      break\n",
        "      \n",
        "    word = ''\n",
        "    \n",
        "    if idx > 0:\n",
        "      \n",
        "      word = idx2word_french[idx]\n",
        "      output_sequence.append(word)\n",
        "      \n",
        "    target[0,0] = idx\n",
        "  \n",
        "  return ' '.join(output_sequence)\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3jX1xRcD9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce2b81e0-d801-4892-d860-866a7ac1ac1d"
      },
      "source": [
        "for _ in range(20):\n",
        "  \n",
        "  i = np.random.choice(len(english_text_input))\n",
        "  \n",
        "  input_sequence = encode_english_input[i:i+1]\n",
        "  \n",
        "  translation = decode_sentences(input_sequence)\n",
        "  \n",
        "  print('-')\n",
        "  \n",
        "  print('Input: ', english_text_input[i])\n",
        "  print('Expected: ', french_text_input[i])\n",
        "  print('Translation: ', translation)\n",
        "  \n",
        "    \n",
        "    \n",
        "  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input:  I love astronomy.\n",
            "Expected:  <sos> J'adore l'astronomie.\n",
            "Translation:  j'adore la musique.\n",
            "-\n",
            "Input:  I'm selfish.\n",
            "Expected:  <sos> Je suis égoïste.\n",
            "Translation:  je suis\n",
            "-\n",
            "Input:  You may go.\n",
            "Expected:  <sos> Tu peux y aller.\n",
            "Translation:  tu peux y aller.\n",
            "-\n",
            "Input:  That was a test.\n",
            "Expected:  <sos> C'était un examen.\n",
            "Translation:  c'était un mensonge.\n",
            "-\n",
            "Input:  Do it again!\n",
            "Expected:  <sos> Faites-le de nouveau !\n",
            "Translation:  faites-le de nouveau !\n",
            "-\n",
            "Input:  I wasn't kidding.\n",
            "Expected:  <sos> Je n'étais pas en train de blaguer.\n",
            "Translation:  je n'ai pas faim.\n",
            "-\n",
            "Input:  Buy me a drink.\n",
            "Expected:  <sos> Payez-moi un coup !\n",
            "Translation:  donnez-moi un verre !\n",
            "-\n",
            "Input:  Do it delicately.\n",
            "Expected:  <sos> Fais-le délicatement.\n",
            "Translation:  faites-le de !\n",
            "-\n",
            "Input:  No taxi stopped.\n",
            "Expected:  <sos> Aucun taxi ne s'est arrêté.\n",
            "Translation:  personne n'est d'un mensonge.\n",
            "-\n",
            "Input:  I feel ashamed.\n",
            "Expected:  <sos> Je me sens honteux.\n",
            "Translation:  je me sens mal.\n",
            "-\n",
            "Input:  Who will win?\n",
            "Expected:  <sos> Qui va gagner ?\n",
            "Translation:  qui va gagné\n",
            "-\n",
            "Input:  I remember that.\n",
            "Expected:  <sos> Je me souviens de ça.\n",
            "Translation:  je le souviens\n",
            "-\n",
            "Input:  He sells cars.\n",
            "Expected:  <sos> Il vend des voitures.\n",
            "Translation:  il adore les chiens.\n",
            "-\n",
            "Input:  That's peculiar.\n",
            "Expected:  <sos> C'est étrange.\n",
            "Translation:  c'est ironique.\n",
            "-\n",
            "Input:  Tom's not here.\n",
            "Expected:  <sos> Tom n'est pas ici.\n",
            "Translation:  tom n'est pas ici.\n",
            "-\n",
            "Input:  Who says that?\n",
            "Expected:  <sos> Qui dit ça ?\n",
            "Translation:  qui a dit ça.\n",
            "-\n",
            "Input:  I'm nervous.\n",
            "Expected:  <sos> Je suis nerveux.\n",
            "Translation:  je suis\n",
            "-\n",
            "Input:  Drive slowly.\n",
            "Expected:  <sos> Conduis lentement.\n",
            "Translation:  \n",
            "-\n",
            "Input:  I work with him.\n",
            "Expected:  <sos> Je travaille avec lui.\n",
            "Translation:  je viens de le maison.\n",
            "-\n",
            "Input:  I'm finicky.\n",
            "Expected:  <sos> Je suis méticuleux.\n",
            "Translation:  je suis je suis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdUHB-7n0Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}