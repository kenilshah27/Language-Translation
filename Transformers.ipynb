{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sQ76VGNmxVg",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "\n",
        "[Original Paper](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n",
        "\n",
        "[Medium Article](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)\n",
        "\n",
        "[Analytics Vidhya Article](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8xziSHJfgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import math\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout,Concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzS3F1o74YBM",
        "colab_type": "code",
        "outputId": "def37d69-30bf-4902-9052-6bb12fcedae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_e92o-J25e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting Parameters\n",
        "\n",
        "BATCH_SIZE = 32            # Batch size for the training set. After each BATCH_SIZE the weights will be updated\n",
        "EPOCHS = 100               # Number of times we will train the model\n",
        "VOCAB_SIZE = 20000         # Vocab size for the dataset\n",
        "EMBEDDING_DIM = 512        # Embedding units to represent a single word in text for both language"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ99Fd0T7drC",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML15kcrqJl5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Preprocessing class for all the sequences.\n",
        "A single instance is created for all the different languages and all the functions will be performed using this class\n",
        "\n",
        "\"\"\"\n",
        "class Preprocessing():\n",
        "  \n",
        "  \"\"\"\n",
        "  Args:\n",
        "    \n",
        "    VOCAB_SIZE = Maximum number of words in the sentence.\n",
        "    length = Size of the longest sentence\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self,VOCAB_SIZE):\n",
        "    self.VOCAB_SIZE = VOCAB_SIZE\n",
        "    self.length = 0\n",
        "  \n",
        "  \"\"\"\n",
        "  Functions:\n",
        "  \n",
        "  Tokenizer_text: It tokenizes the corpus passed to it and then return the sequences, the word to index mapping, length of the largest sentence and the index to word mapping.\n",
        "  total_words: Returns the total number of words in the vocabulary\n",
        "  padding: Returns a padded sequence with the maxlen equal to self.length\n",
        "  \n",
        "  \"\"\"\n",
        "  def Tokenizer_text(self,text,VOCAB_SIZE,filters = None):\n",
        "  \n",
        "    if filter is not None:\n",
        "    \n",
        "      self.tokenizer_lang = Tokenizer(num_words=VOCAB_SIZE,filters = '')\n",
        "      self.sentences = self.tokenizer_lang.fit_on_texts(text)\n",
        "      self.sequences = self.tokenizer_lang.texts_to_sequences(text)\n",
        "      self.word2idx = self.tokenizer_lang.word_index                           # Indexing happen in decreasing order of the frequency of the word. Most frequent word is indexed first.\n",
        "  \n",
        "    else:\n",
        "      \n",
        "      self.tokenizer_lang = Tokenizer(num_words=VOCAB_SIZE)\n",
        "      self.sentences = self.tokenizer_lang.fit_on_texts(text)\n",
        "      self.sequences = self.tokenizer_lang.texts_to_sequences(text)\n",
        "      self.word2idx = self.tokenizer_lang.word_index\n",
        "      \n",
        "    self.length = max(len(s) for s in self.sequences)\n",
        "    self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
        "    \n",
        "    return self.sequences,self.word2idx,self.length,self.idx2word\n",
        "  \n",
        "  \"\"\"\n",
        "  eng = Preprocessing(VOCAB_SIZE)\n",
        "  text = ['How are you','I am good thankyou']\n",
        "  eng.Tokenizer_text(text,VOCAB_SIZE)\n",
        "  \n",
        "  OUTPUT \n",
        "  \n",
        "  SEQUENCES =   ([[1, 2, 3], [4, 5, 6, 7]],\n",
        "  WORD2INDEX =  {'am': 5, 'are': 2, 'good': 6, 'how': 1, 'i': 4, 'thankyou': 7, 'you': 3},\n",
        "  MAX SEQUENCE LENGTH =  4,\n",
        "  INDEX2WORD =  {1: 'how', 2: 'are', 3: 'you', 4: 'i', 5: 'am', 6: 'good', 7: 'thankyou'})\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def total_words(self):\n",
        "    return len(self.word2idx)\n",
        "    \n",
        "  \"\"\"\n",
        "  eng.total_words()\n",
        "  \n",
        "  OUTPUT\n",
        "  \n",
        "  7\n",
        "  \n",
        "  \"\"\"\n",
        "  def padding(self):\n",
        "    return pad_sequences(self.sequences,maxlen = self.length,padding = 'post')\n",
        "  \n",
        "  \"\"\"\n",
        "  eng.padding()\n",
        "  \n",
        "  OUTPUT\n",
        "  \n",
        "  array([[1, 2, 3, 0],\n",
        "         [4, 5, 6, 7]], dtype=int32)\n",
        "  \n",
        "  \"\"\"\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHDjbbam7mxB",
        "colab_type": "text"
      },
      "source": [
        "# EMBEDDING LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TTvJhqU7rTp",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czWC0lxrKTDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Embedding Class.\n",
        "The embedding of each vector will be learned during the model execution and will be learned as the parameters of the model\n",
        "We call also use a predefined embedding like the one provided in GloVe\n",
        "\n",
        "\"\"\"\n",
        "class Embedder(nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Args\n",
        "    num_words is the maximum number of words present in the vocabulary. Is equal to the value returned by total_words from the Preprocessing cards. \n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, num_words, embed_dim):\n",
        "      super().__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.embed = nn.Embedding(num_words, self.embed_dim)\n",
        "    \n",
        "    \"\"\"\n",
        "    Args\n",
        "    x is the input i.e. the sentence with the words in the number format\n",
        "    \n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "      x =  self.embed(x)\n",
        "      return x\n",
        "    \n",
        "    \"\"\"\n",
        "    c = Embedder(7,4)   # We take num_words to be 7 because we have 6 numbers from 1-6 and 0 for padding so total 7\n",
        "    text = torch.LongTensor([[1,2,3],[4,5,6]]) \n",
        "    c(text)\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    tensor([[[-0.1336,  0.6301,  0.7810, -0.8898],\n",
        "             [ 0.3597,  1.9011,  0.6907, -0.9391],\n",
        "             [-0.8484, -0.1596,  1.4215,  1.9027]],\n",
        "\n",
        "            [[ 0.3623, -0.6805, -0.7152, -0.3026],\n",
        "             [-1.3681, -0.0394,  0.2179, -0.5563],\n",
        "             [ 1.1576, -0.8182,  0.4053,  0.2688]]], grad_fn=<EmbeddingBackward>)\n",
        "             \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3BtKiAc7upN",
        "colab_type": "text"
      },
      "source": [
        "## Position Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBR7DpDckT6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Position Embedding.\n",
        "Since we are not using a RNN to create this transformer. The network would have no idea about the position of the word.\n",
        "Hence we add some information related to the position to this embedding layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  embed_dim is the dimension length for each of the words generated by the Embedding Layer\n",
        "  max_seq_len is the max length of the sequence. This is equal to the length variable for each of the preprocessed text\n",
        "    \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,embed_dim,max_seq_len,dropout = 0.1):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    self.positionencoding = torch.zeros((self.max_seq_len,self.embed_dim))  # So that we have a structure for each position in relation to each dimension column\n",
        "    \n",
        "    \"\"\"\n",
        "    We first start with each position followed by each dimension column and then generate a value for that column which would then be added to the embedding output.\n",
        "    \"\"\"\n",
        "     \n",
        "    for position_in_sequence in range(self.max_seq_len):              \n",
        "      for position_in_dimension in range(0, self.embed_dim, 2):\n",
        "        self.positionencoding[position_in_sequence, position_in_dimension] = math.sin(position_in_sequence / (10000 ** ((2 * position_in_dimension)/self.embed_dim)))\n",
        "        self.positionencoding[position_in_sequence, position_in_dimension + 1] = math.cos(position_in_sequence / (10000 ** ((2 * (position_in_dimension + 1))/self.embed_dim)))\n",
        "                \n",
        "    self.positionencoding = self.positionencoding.unsqueeze(0)   \n",
        "    # So that we can have the nnumber of dimensions same as the input data. the first dimensions would overlap the batchsize so the next two dimensions overlap seq_len and embed_dim\n",
        "    \n",
        "  \"\"\"\n",
        "  Args\n",
        "  x is the input i.e. the sentence with the words in the embeded format\n",
        "  \"\"\"      \n",
        "  \n",
        "  def forward(self,x):   # The x is the embedding output\n",
        "    \n",
        "    x = x * math.sqrt(self.embed_dim)   # Giving a higher value to the embedding output\n",
        "    \n",
        "    seq_len = x.size(1)          # Get the sequence length as the shape of x would be (batch_size,seq_len,embed_dim)\n",
        "    self.positionencoding = Variable(self.positionencoding[:,:seq_len],requires_grad=False)  # Adding the Position\n",
        "    x = x + self.positionencoding\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "    \n",
        "    \"\"\"\n",
        "    SELF NOTE\n",
        "    \n",
        "    All of this gives the same output\n",
        "    \n",
        "     self.positioneoncodeing[:,:seq_len]\n",
        "     self.positioneoncodeing[:,:,:]\n",
        "     self.positioneoncodeing[:,:self.seq_len]\n",
        "     self.positioneoncodeing[:,:self.seq_len,:self.embed_dim]\n",
        "     \n",
        "    Assuming the embed_dim and max_seq_len we gave has the same values as the values of the dimension of positionencoding  \n",
        "    \n",
        "    OUTPUTS\n",
        "    \n",
        "    x = torch.zeros(2,3,4)\n",
        "    layer = PositionEmbedding(4,3)\n",
        "    c = layer(x)\n",
        "    c                   # For each batch size\n",
        "    \n",
        "    tensor([[[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00],\n",
        "             [8.4147e-01, 9.9995e-01, 1.0000e-04, 1.0000e+00],\n",
        "             [9.0930e-01, 9.9980e-01, 2.0000e-04, 1.0000e+00]],\n",
        "\n",
        "           [[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00],\n",
        "            [8.4147e-01, 9.9995e-01, 1.0000e-04, 1.0000e+00],\n",
        "            [9.0930e-01, 9.9980e-01, 2.0000e-04, 1.0000e+00]]])\n",
        "    \n",
        "    c[0]               # For each input whose seq length is 3 and embed_dim is 4\n",
        "    \n",
        "    tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00],\n",
        "        [8.4147e-01, 9.9995e-01, 1.0000e-04, 1.0000e+00],\n",
        "        [9.0930e-01, 9.9980e-01, 2.0000e-04, 1.0000e+00]])\n",
        "        \n",
        "    c[0,1]             # For each input and word at time 1 we get the followind embed_dim values\n",
        "    \n",
        "    tensor([8.4147e-01, 9.9995e-01, 1.0000e-04, 1.0000e+00])\n",
        "    \"\"\"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev-rw_6R7yOX",
        "colab_type": "text"
      },
      "source": [
        "# MASKING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzt3wEfcvSO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Masking\n",
        "\n",
        "This will mask all the padded values in the sequence for source and target so that they do not contribute to the information of the next word\n",
        "\"\"\"\n",
        "\n",
        "class Masking():\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  sequencetype to see if its a source or target since both of them have different maskings\n",
        "  paddingvalue = 0 to provide the integer for padding\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self,sequencetype = 'Source',paddingvalue = 0):\n",
        "    \n",
        "    self.sequencetype = sequencetype\n",
        "    self.paddingvalue = paddingvalue\n",
        "  \n",
        "  def create_mask(self,sequence):  # Sequence is the input sequence\n",
        "\n",
        "    if self.sequencetype == 'Source':   # Encoder Mapping or input Mapping\n",
        "      \n",
        "      self.mask = (sequence != self.paddingvalue).unsqueeze(-2)   # We unsqueeze the data so that for each input we have a mask of size (1,seq_len) \n",
        "      return self.mask  \n",
        "    \n",
        "      \"\"\"\n",
        "    \n",
        "        c = Masking(sequencetype = 'Target',paddingvalue = 0)\n",
        "        text = torch.Tensor([[1,2,3,4,0,0],[1,2,3,4,0,0]])\n",
        "        c.create_mask(text)\n",
        "      \n",
        "        OUTPUT WHEN mask is printed\n",
        "       \n",
        "        tensor([[1, 1, 1, 1, 0, 0],\n",
        "               [1, 1, 1, 1, 0, 0]], dtype=torch.uint8)\n",
        "       \n",
        "      \"\"\"   \n",
        "    \n",
        "    else:\n",
        "      self.mask = (sequence != self.paddingvalue).unsqueeze(-2)  # This is done to avoid padding\n",
        "      \n",
        "      # We also need to make sure that at no point in time, there is a leftward movement of information in the decoder. i.e. No word at time greater than t is able to impact the decision at time t.\n",
        "      \n",
        "      size = sequence.size(1)   # Since the shape of the sequence is batch_size,seq_len,embed_dim\n",
        "      \n",
        "      self.forwardmask = np.ones((1,size,size))\n",
        "      self.forwardmask = np.triu(self.forwardmask,k=1).astype('uint8')   # This will make all the values below the kth diagonal zero \n",
        "      \n",
        "      self.nopeak_mask = Variable(torch.from_numpy(self.forwardmask) == 0)   # This will basically reverse the matrix with 1 becoming zero and zero becoming one. Hence we get our required masking for the sequences,\n",
        "      \n",
        "      \"\"\"\n",
        "      c = Masking(sequencetype = 'Source',paddingvalue = 0)\n",
        "      text = torch.Tensor([[1,2,3,4,0,0],[1,2,3,4,0,0]])\n",
        "      c.create_mask(text)\n",
        "      \n",
        "      OUTPUT WHEN nopeak_mask is printed\n",
        "\n",
        "         tensor([[[1, 0, 0, 0, 0, 0],\n",
        "                  [1, 1, 0, 0, 0, 0],\n",
        "                  [1, 1, 1, 0, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0],\n",
        "                  [1, 1, 1, 1, 1, 0],\n",
        "                  [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
        "\n",
        "      As we can see for t = 1 all the values in the future is masked. For t = 2 only value at t = 1 is unmasked and so on.\n",
        "      \n",
        "      But we can see that we get this output only once but we need this masking matrix for all the inputs. We also need the masking for the padding values.\n",
        "      Hence the next step.\n",
        "      \n",
        "      \"\"\"\n",
        "      \n",
        "      self.final_mask = self.mask & self.nopeak_mask \n",
        "      \n",
        "      \"\"\"\n",
        "    \n",
        "       c = Masking(sequencetype = 'Target',paddingvalue = 0)\n",
        "       text = torch.Tensor([[1,2,3,4,0,0],[1,2,3,4,0,0]])\n",
        "       c.create_mask(text)\n",
        "      \n",
        "       OUTPUT WHEN final_mask is printed\n",
        "\n",
        "         tensor([[[1, 0, 0, 0, 0, 0],\n",
        "                  [1, 1, 0, 0, 0, 0],\n",
        "                  [1, 1, 1, 0, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0]],\n",
        "                  \n",
        "                  \n",
        "                 [[1, 0, 0, 0, 0, 0],\n",
        "                  [1, 1, 0, 0, 0, 0],\n",
        "                  [1, 1, 1, 0, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0],\n",
        "                  [1, 1, 1, 1, 0, 0]]], dtype=torch.uint8)\n",
        "\n",
        "      Now we get output for all the inputs. \n",
        "      \n",
        "      \"\"\"\n",
        "      \n",
        "      return self.final_mask\n",
        "      \n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai_6GzBJ72em",
        "colab_type": "text"
      },
      "source": [
        "# ATTENTION LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBRUq3ig74K9",
        "colab_type": "text"
      },
      "source": [
        "## Multi Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIGSulHZbkgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Multi Headed Attention Layer\n",
        "\n",
        "Here we will implement the Multi Headed Attention class which will calculate the self attention for the encoder and the decoder in the architecture. To learn more about how this works look at the references mentioned above.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  heads is the number of heads we want to create as per the paper\n",
        "  embed_dim is the embedding dimension\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self,heads,embed_dim,dropout = 0.1): # You can change the dropout value to include regularization\n",
        "    super().__init__()\n",
        "    \n",
        "    self.heads = heads\n",
        "    self.embed_dim = embed_dim\n",
        "    self.matrixsize = self.embed_dim // self.heads  # So that size is in int and not in float\n",
        "    \n",
        "    self.q_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.v_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    \n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    self.out = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    \n",
        "  \"\"\"\n",
        "  Args\n",
        "  q_vector,k_vector,v_vector are the vector provided for the multi headed attention\n",
        "  mask is the mask for the vectors\n",
        "  \n",
        "  \"\"\"\n",
        "  def forward(self, q_vector, k_vector, v_vector, mask=None):\n",
        "      \n",
        "    self.batch_size = q_vector.size(0)\n",
        "      \n",
        "    self.q_vector = self.q_linear(q_vector).view(self.batch_size, -1, self.heads, self.matrixsize)                # Here we calculate different values of matrix Q,K,V\n",
        "    self.k_vector = self.k_linear(k_vector).view(self.batch_size, -1, self.heads, self.matrixsize)\n",
        "    self.v_vector = self.v_linear(v_vector).view(self.batch_size, -1, self.heads, self.matrixsize)\n",
        "    \n",
        "    \"\"\"\n",
        "    q = torch.zeros(2,20,20)\n",
        "    c = MultiHeadedAttention(5,20)\n",
        "    c(q,q,q).size()\n",
        "    \n",
        "    When the .view lines and the transpose line below were commented and the size of q_vector was printed.\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    torch.Size([2, 20, 20]) which would have values different from the ones passed as a linear function was applied\n",
        "    \n",
        "    q = torch.zeros(2,20,20)\n",
        "    c = MultiHeadedAttention(5,20)\n",
        "    c(q,q,q).size()\n",
        "    \n",
        "    When the transpose line below were commented and the size of q_vector was printed.\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    Size of the q vector\n",
        "    \n",
        "    torch.Size([2, 20, 5, 4])\n",
        "    \n",
        "    The third dimension in the input is the embed_dim (20 in our example). Now we have split that dimension into 5 heads each with a size of 4. We need to transpose this into a form which we can use for multiplication.\n",
        "    Hence the transpose layer\n",
        "    \n",
        "    \"\"\"\n",
        "       \n",
        "    self.q_vector = self.q_vector.transpose(1,2)            \n",
        "    self.k_vector = self.k_vector.transpose(1,2)\n",
        "    self.v_vector = self.v_vector.transpose(1,2)\n",
        "    \n",
        "    \"\"\"\n",
        "    q = torch.zeros(2,20,20)\n",
        "    c = MultiHeadedAttention(5,20)\n",
        "    c(q,q,q).size()\n",
        "    \n",
        "    When the .view lines and the transpose line below were commented and the size of q_vector was printed.\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    Size of the q vector\n",
        "    \n",
        "    torch.Size([2, 5, 20, 4])\n",
        "       \n",
        "    \"\"\"\n",
        "    \n",
        "      \n",
        "    # Now we will calculate individual attentions\n",
        "    scores = single_attention(self.q_vector, self.k_vector, self.v_vector, self.matrixsize, mask, self.dropout)\n",
        "    \n",
        "    \"\"\"\n",
        "    We get the scores shape as (2,5,20,4)\n",
        "    \n",
        "    We have done the matrix multiplication now we need to normalize and concat the values\n",
        "    \"\"\"\n",
        "    \n",
        "    # This is the concat layer for each of the attentions\n",
        "    concat = scores.transpose(1,2).contiguous().view(self.batch_size, -1, self.embed_dim)\n",
        "        \n",
        "    output = self.out(concat)    # This is the final Linear Layer\n",
        "    \n",
        "    \"\"\"\n",
        "    Shape of the output\n",
        "    \n",
        "    torch.Size([2, 20, 20])\n",
        "    \n",
        "    \"\"\"\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vrVcUXh777l",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWxT1tMwsFoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Single attention Layer.\n",
        "\n",
        "This calculate a single attention for the dataset.\n",
        "It calculate the function\n",
        "\n",
        "(q_vector*k_vector)*v_vector/sqrt\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Args\n",
        "\n",
        "q_vector,k_vector,v_vector are passed from the above Multi headed attention class\n",
        "embed_dim is the embedding dimesion\n",
        "mask is the mask for the input to single attetion\n",
        "\n",
        "\"\"\"\n",
        "def single_attention(q_vector, k_vector, v_vector, matrixsize, mask=None, dropout=None):\n",
        "  \n",
        "  scores = torch.matmul(q_vector, k_vector.transpose(-2, -1)) /  math.sqrt(matrixsize)   # Size of the k_vector.transpose is [2,5,4,20]\n",
        "  \n",
        "  \"\"\"\n",
        "  Size of the scores\n",
        "  \n",
        "  OUTPUT\n",
        "  torch.Size([2, 5, 20, 20])\n",
        "  \n",
        "  \"\"\"\n",
        "  if mask is not None:\n",
        "    mask = mask.unsqueeze(1)                        \n",
        "    \n",
        "    \"\"\"\n",
        "    Scores are of size (2,5,20,20)\n",
        "    But if we would have applied the mask function for the same values it would be of size (2,20,20) since heads are created as a part of the multiheaded attention function\n",
        "    \n",
        "    So we need to unsqueeze it at dim = 1 so the new shape of the mask is (2,1,20,20). Hence we get a mask for each of the values\n",
        "    \n",
        "    \"\"\"\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  \n",
        "  scores = F.softmax(scores, dim=-1) # We apply the softmax on the last dim\n",
        "    \n",
        "  if dropout is not None:\n",
        "    scores = dropout(scores)\n",
        "        \n",
        "  output = torch.matmul(scores, v_vector)\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  Shape of scores is (2,5,20,20)\n",
        "  Shape of v_vector is (2,5,20,4)\n",
        "  \n",
        "  Hence after matrix multiplication\n",
        "  Shape of output is (2,5,20,4)\n",
        "  \"\"\"\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6rqKgqb7--B",
        "colab_type": "text"
      },
      "source": [
        "# FEED FORWARD LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZNfEEng1k2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Feed Forward Network\n",
        "\n",
        "As per the paper each layer in the encoder and decoder contains a fully connected feedforward layer with a relu function in between. This clas is use to model this layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  embed_dim is the embedding dimension\n",
        "  ffhd is the feed forward network hidden layer size\n",
        "  \"\"\"\n",
        "  def __init__(self,embed_dim,ffhd = 2048,dropout = 0.1):  # ffhd stands for Feed Forward Hidden Dimension \n",
        "    super().__init__()\n",
        "    \n",
        "    self.linear_1 = nn.Linear(embed_dim,ffhd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(ffhd,embed_dim)\n",
        "    \n",
        "  \"\"\"\n",
        "  Args\n",
        "  x is the input\n",
        "  \"\"\"\n",
        "  def forward(self,x):\n",
        "    \n",
        "    x = F.relu(self.linear_1(x))\n",
        "    \n",
        "    \"\"\"\n",
        "    c = FeedForward(20)\n",
        "    t = torch.ones(2,20,20)\n",
        "    c(t).size()\n",
        "    \n",
        "    Shape before first linear layer : (2,20,20) # As per what was calculated in the MultiHeaded Attention class\n",
        "    Shape after first linear layer : (2,20,2048)\n",
        "    \"\"\"\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    x = self.linear_2(x)\n",
        "    \n",
        "    \"\"\"\n",
        "    c = FeedForward(20)\n",
        "    t = torch.ones(2,20,20)\n",
        "    c(t).size()\n",
        "    \n",
        "    Shape before second linear layer : (2,20,2048) \n",
        "    Shape after second linear layer : (2,20,20)\n",
        "    \"\"\"\n",
        "    \n",
        "    return x\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMov3OOR8BaW",
        "colab_type": "text"
      },
      "source": [
        "# NORMALIZATION LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dfcjheT3Lm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Normalization Layer\n",
        "\n",
        "Each of the output from the feed forward is followed by a normalization layer so that the outputs do not go out of hand and stay with a certain level\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Normalization(nn.Module):\n",
        "  \"\"\"\n",
        "  Args\n",
        "  embed_dim is the emebedding size\n",
        "  \"\"\"\n",
        "  def __init__(self, embed_dim, eps = 1e-6):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # create two learnable parameters to calibrate normalisation\n",
        "    \n",
        "    self.alpha = nn.Parameter(torch.ones(self.embed_dim))\n",
        "    self.bias = nn.Parameter(torch.zeros(self.embed_dim))\n",
        "    self.eps = eps\n",
        "  \n",
        "  def forward(self,x):\n",
        "    \n",
        "    norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "    return norm\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnyRrbVT7P1w",
        "colab_type": "text"
      },
      "source": [
        "# PUTTING IT ALL TOGETHER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2m5mfco8ZWm",
        "colab_type": "text"
      },
      "source": [
        "As we see in the paper the encoder and the decoder have a particular architecture comprising of many layers. And this layers have a particular structure. We will create this strcuture here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vG7sqN-8V9t",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Layer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTtVNv48Fnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Encoder Structure\n",
        "\n",
        "Each encoder has a single Multi Headed Attention layer and a Feed Forward Layer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class EncoderStructure(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  embed_dim is the embedding dimesion\n",
        "  heads is the number of heads for the multiheaded attention layer\n",
        "  \"\"\"\n",
        "  def __init__(self,embed_dim,heads,dropout = 0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    # This layer are in order as shown in the image in the paper\n",
        "    \n",
        "    self.normalize_1 = Normalization(embed_dim = embed_dim)\n",
        "    self.mhattention = MultiHeadedAttention(embed_dim = embed_dim, heads = heads)\n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.normalize_2 = Normalization(embed_dim = embed_dim)\n",
        "    self.feedforward = FeedForward(embed_dim=embed_dim)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "    \n",
        "  \"\"\"\n",
        "  Args\n",
        "  x is the input size\n",
        "  mask is the encoder mask\n",
        "  \"\"\"\n",
        "  def forward(self,x,mask):\n",
        "    \n",
        "    x2 = self.normalize_1(x)                                 # This is like the calculation of q,k,v which are equal\n",
        "    x = x + self.dropout_1(self.mhattention(x2,x2,x2,mask))  # This calculates the multi headed attention and then perform the add and norm layer as per the paper\n",
        "    x2 = self.normalize_1(x)                                 # This applies a normalization before the feedforward layer\n",
        "    x = x2 + self.dropout_2(self.feedforward(x2))            # This is the feedforward layer followed by the add and norm layer as per the paper\n",
        "    \n",
        "    return x\n",
        "    \n",
        "    \"\"\"\n",
        "    c = EncoderStructure(20,5)\n",
        "    t = torch.ones(2,20,20)\n",
        "    c(t,None).size()\n",
        "    \n",
        "    Where None is mentioned we provide the mask for the dataset\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    torch.Size([2, 20, 20])\n",
        "    \n",
        "    The shape is same as the input but all the functions of the encoder structure are applied to it.\n",
        "    \n",
        "    \"\"\"\n",
        "  \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVITs11yABuC",
        "colab_type": "text"
      },
      "source": [
        "## Decoder Layer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5HKlrfYAEkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Decoder Structure\n",
        "\n",
        "Each decoder has two Multi Headed Attention layer and a Feed Forward Layer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class DecoderStructure(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  embed_dim is the embedding dimesion\n",
        "  heads is the number of heads for the multiheaded attention layer\n",
        "  \"\"\"\n",
        "  def __init__(self,embed_dim,heads,dropout = 0.1):\n",
        "    super().__init__()\n",
        "  \n",
        "    # This layer are in order as shown in the image in the paper\n",
        "  \n",
        "    self.normalize_1 = Normalization(embed_dim = embed_dim)\n",
        "    self.attention_1 = MultiHeadedAttention(embed_dim = embed_dim,heads = heads)\n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "  \n",
        "    self.normalize_2 = Normalization(embed_dim = embed_dim)\n",
        "    self.attention_2 = MultiHeadedAttention(embed_dim = embed_dim, heads = heads)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "  \n",
        "    self.normalize_3 = Normalization(embed_dim = embed_dim)\n",
        "    self.feedforward = FeedForward(embed_dim = embed_dim)\n",
        "    self.dropout_3 = nn.Dropout(dropout)\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  x is the input size\n",
        "  encoder_output is the encoder output\n",
        "  source_mask is the source mask\n",
        "  target_mask is the target_mask\n",
        "  \"\"\"  \n",
        "  def forward(self,x,encoder_output,source_mask,target_mask):\n",
        "     \n",
        "    x2 = self.normalize_1(x)                                              # This is like the calculation of q,k,v which are equal\n",
        "    x = x + self.dropout_1(self.attention_1(x2,x2,x2,target_mask))       # This calculates the multi headed attention and then perform the add and norm layer as per the paper and uses the target mask\n",
        "    \n",
        "    x2 = self.normalize_2(x)                                              # This is like the calculation of q,k,v which are equal\n",
        "    x = x + self.dropout_1(self.attention_2(x2,encoder_output,encoder_output,source_mask))\n",
        "    \n",
        "    # This layer is the second multi headed layer from the image. It has two inputs k and v coming from the encoder layer and one input coming from the previous multi headed attention.\n",
        "    # This makes use of the source mask as we have inputs coming from the encoder\n",
        "    \n",
        "    x2 = self.normalize_3(x)\n",
        "    x = x + self.dropout_3(self.feedforward(x2))\n",
        "    \n",
        "    return x\n",
        "  \n",
        "    \"\"\"\n",
        "    c = DecoderStructure(20,5)\n",
        "    t = torch.ones(2,20,20)\n",
        "    c(t,t,None,None).size()       \n",
        "    \n",
        "    Passing the same t inplace of the encoder output just to check the shapes\n",
        "    \n",
        "    Where None is mentioned we provide the mask for the dataset\n",
        "    \n",
        "    OUTPUT\n",
        "    \n",
        "    torch.Size([2, 20, 20])\n",
        "    \n",
        "    The shape is same as the input but all the functions of the encoder structure are applied to it.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemMO3V0SEJB",
        "colab_type": "text"
      },
      "source": [
        "## Getting Layer Copies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiHd60PWQHJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module,number):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(number)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy8A7bxQyFZ",
        "colab_type": "text"
      },
      "source": [
        "##  Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPfZ0aUgQmq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Encoder\n",
        "\n",
        "In the above classes we just build one Encoder Layer structure. This class will create the entire structure for the Encoder using the Layer architecture created above.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  num_words is the total word size for the vacab\n",
        "  embed_dim is the embedding dimension\n",
        "  number is the number of encoder layers\n",
        "  heads is the number of heads for the attention layer\n",
        "  max_seq_len is the maximum length of the input to encoder\n",
        "  \"\"\"\n",
        "  def __init__(self,num_words,embed_dim,number,heads,max_seq_len):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.number = number                 # Number of Encoder Layers\n",
        "    self.embedded_layer = Embedder(embed_dim=embed_dim,num_words=num_words)                                # First creating Word Embedding the input \n",
        "    self.PositionEmbedding = PositionEmbedding(embed_dim=embed_dim,max_seq_len=max_seq_len)                # Creating the Position Embedding\n",
        "    self.encoderlayers = clones(module = EncoderStructure(embed_dim=embed_dim,heads=heads),number=number)  # Creating the Encoder Layers\n",
        "    self.normalization = Normalization(embed_dim=embed_dim)                                                # Normalization LAyer\n",
        "    \n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  source_input is the input to encoder\n",
        "  source_mask is the mask for the input\n",
        "  \"\"\"\n",
        "  def forward(self,source_input,source_mask):\n",
        "    \n",
        "    x = self.embedded_layer(source_input)\n",
        "    x = self.PositionEmbedding(x)\n",
        "    \n",
        "    for i in range(self.number):  \n",
        "      x = self.encoderlayers[i](x,source_mask)\n",
        "    \n",
        "    x = self.normalization(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  \"\"\"\n",
        "  c = Encoder(20,20,5,5,5)\n",
        "  m = Masking(sequencetype='Source',paddingvalue=0)\n",
        "  t = torch.LongTensor([[1,2,3,4,5],[6,7,8,9,10]])\n",
        "  print('Input Shape',t.size())\n",
        "  a = m.create_mask(t)\n",
        "  c(t,a).size()\n",
        "  print('Output Shape',c(t,a).size())\n",
        "\n",
        "  We created an encoder structure with num_words = 20, embed_Dim = 20,number of encoder layers = 5,heads = 5,max_seq_len = 5\n",
        "  We then create a masking for the input t which basically represents a sentence and its 5 words \n",
        "  Then we pass it to the Encoder layer as it is the input it expects\n",
        "  \n",
        "  OUTPUT\n",
        "  \n",
        "  Input Shape torch.Size([2, 5])\n",
        "  Output Shape torch.Size([2, 5, 20])\n",
        "  \n",
        "  As we can see that we are able to create an embedding of 20 for each words and perform all the tasks for the encoder.\n",
        "  \n",
        "  \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-33QRJRaXKO",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzAJSJccXdAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Decoder\n",
        "\n",
        "In the above classes we just build one Decoder Layer structure. This class will create the entire structure for the Decoder using the Layer architecture created above.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  num_words is the total word size for the vacab\n",
        "  embed_dim is the embedding dimension\n",
        "  number is the number of decoder layers\n",
        "  heads is the number of heads for the attention layer\n",
        "  max_seq_len is the maximum length of the input to decoder\n",
        "  \"\"\"\n",
        "  def __init__(self,num_words,embed_dim,number,heads,max_seq_len):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.number = number                 # Number of Encoder Layers\n",
        "    self.embedded_layer = Embedder(embed_dim=embed_dim,num_words=num_words)                                # First creating Word Embedding the input \n",
        "    self.PositionEmbedding = PositionEmbedding(embed_dim=embed_dim,max_seq_len=max_seq_len)                # Creating the Position Embedding\n",
        "    self.decoderlayers = clones(module = DecoderStructure(embed_dim=embed_dim,heads=heads),number=number)  # Creating the Encoder Layers\n",
        "    self.normalization = Normalization(embed_dim=embed_dim)                                                # Normalization LAyer\n",
        "    \n",
        "  \"\"\"\n",
        "  Args\n",
        "  target_input is the input to decoder\n",
        "  encoder_input is the output of the encoder\n",
        "  source_mask is the mask for the encoder\n",
        "  target_mask is the mask for the decoder\n",
        "  \"\"\"\n",
        "  def forward(self,target_input,encoder_input,source_mask,target_mask):\n",
        "    \n",
        "    x = self.embedded_layer(target_input)\n",
        "    x = self.PositionEmbedding(x)\n",
        "    \n",
        "    for i in range(self.number):  \n",
        "      x = self.decoderlayers[i](x,encoder_input,source_mask,target_mask)\n",
        "    \n",
        "    x = self.normalization(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  \"\"\"\n",
        "  c = Encoder(20,20,5,5,5)\n",
        "  m = Masking(sequencetype='Source',paddingvalue=0)\n",
        "  t = torch.LongTensor([[1,2,3,4,5],[6,7,8,9,10]])\n",
        "  print('Input Shape',t.size())\n",
        "  a = m.create_mask(t)\n",
        "  ei = c(t,a) \n",
        "  print('Output Shape Encoder',ei.size())\n",
        "  d = Decoder(20,20,5,5,5)\n",
        "  x = d(t,ei,a,a)\n",
        "  print('Output Shape Decoder',x.size())\n",
        "\n",
        "  We created an encoder structure with num_words = 20, embed_Dim = 20,number of encoder layers = 5,heads = 5,max_seq_len = 5\n",
        "  We then create a masking for the input t which basically represents a sentence and its 5 words \n",
        "  Then we pass it to the Encoder layer as it is the input it expects\n",
        "  \n",
        "  We created an decoder structure with num_words = 20, embed_Dim = 20,number of encoder layers = 5,heads = 5,max_seq_len = 5\n",
        "  For understanding purposes we are using the same mask for source and target but they will be different in the actual set\n",
        "  We then call the Decoder forward function and pass the necessary inputs\n",
        "  \n",
        "  OUTPUT\n",
        "  \n",
        "  Input Shape torch.Size([2, 5])\n",
        "  Output Shape Encoder torch.Size([2, 5, 20])\n",
        "  Output Shape Decoder torch.Size([2, 5, 20])\n",
        "  \n",
        "  As we can see that we are able to create an embedding of 20 for each words and perform all the tasks for the encoder.\n",
        "  \n",
        "  \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gcagG2TdBwL",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rYEXlaabed_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Transformer\n",
        "\n",
        "After creating the Encoder and Decoder Layers. They still are not related as they are individual classes. In this class we will create a sequential flow of this layers\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  \n",
        "  source_vocab is the total words for the source language\n",
        "  target_vocab is the total words for the target language\n",
        "  embed_dim is the embedding dimension\n",
        "  number is the number of encoder and decoder layers\n",
        "  heads is the number of head for the multi attention layer\n",
        "  max_seq_length_source is the maximum length for the source language\n",
        "  max_seq_length_target is the maximum length for the target language\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,source_vocab,target_vocab,embed_dim,number,heads,max_seq_length_source,max_seq_length_target):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.Encoder = Encoder(embed_dim=embed_dim,number=number,num_words=source_vocab,max_seq_len=max_seq_length_source,heads=heads)     # Encoder Layer\n",
        "    self.Decoder = Decoder(embed_dim=embed_dim,number=number,num_words=target_vocab,max_seq_len=max_seq_length_target,heads=heads)    # Decoder Layer\n",
        "    self.outputlayer = nn.Linear(embed_dim,target_vocab)  # Convert an input from embed_dim length to target_vocab length\n",
        "    \n",
        "  \"\"\"\n",
        "  source is the source language sequences\n",
        "  target is the target language sequences\n",
        "  source_mask is the mask for the source\n",
        "  target_mask is the mask for the target\n",
        "  \n",
        "  \"\"\"\n",
        "  def forward(self,source,target,source_mask,target_mask):\n",
        "    \n",
        "    encoder_output = self.Encoder(source,source_mask)\n",
        "    decoder_output = self.Decoder(target,encoder_output,source_mask,target_mask)\n",
        "    output = self.outputlayer(decoder_output)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "  \"\"\"\n",
        "  source_masking = Masking(sequencetype='Source',paddingvalue=0)\n",
        "  source = torch.LongTensor([[1,2,3,4,5],[6,7,8,9,10]])\n",
        "  print('Input Shape',source.size())\n",
        "  source_masked = source_masking.create_mask(source)\n",
        "  c = Transformer(20,25,20,5,5,5,7)\n",
        "  target = torch.LongTensor([[1,2,3,4,5,6,7],[6,7,8,9,10,13,14]])\n",
        "  target_masking = Masking(sequencetype='Target',paddingvalue=0)\n",
        "  target_masked = target_masking.create_mask(target)\n",
        "  c(source,target,source_masked,target_masked).size()\n",
        "  \n",
        "  First we will create two masks one for source and one for target\n",
        "  We will mask our source and target\n",
        "  Then we will run the transformer\n",
        "  \n",
        "  OUTPUT:\n",
        "  \n",
        "  Input Shape torch.Size([2, 5])\n",
        "  torch.Size([2, 7, 25])\n",
        "  \n",
        "  As we can see that the input is successfully giving an output as expected. We said that the max sequence length for target is 7\n",
        "  and it has 25 vocab size. Hence for each input we get the required output.\n",
        "  \n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UICT-4nGlvb_",
        "colab_type": "text"
      },
      "source": [
        "Now as all the strutures are connected and we are able to run on this small inputs. Its time we take the actual inputs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg0j-k7oRios",
        "colab_type": "text"
      },
      "source": [
        "# DATA GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHtephLHRmmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "#imdbdata = pd.read_csv('fra.txt',encoding = 'latin-1')\n",
        "df = pd.read_table('/content/drive/My Drive/Colab Notebooks/Dataset/fra.txt',header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id9YBZtt0ovJ",
        "colab_type": "code",
        "outputId": "6506f800-546e-48d7-c36c-239dbf7bf51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.columns = ['English','French']\n",
        "df.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English    French\n",
              "0     Go.      Va !\n",
              "1     Hi.   Salut !\n",
              "2     Hi.    Salut.\n",
              "3    Run!   Cours!\n",
              "4    Run!  Courez!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6eHUb8zR7mU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_text_input = df['English'][:25000]    # Input for the training Encoder\n",
        "french_text_input = df['French'][:25000].apply(lambda x: '<sos> ' + x + ' <eos>')      # Input for the training Decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dOcwQ8iS-gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing for English Text\n",
        "\n",
        "en_preprocessor = Preprocessing(VOCAB_SIZE=VOCAB_SIZE) \n",
        "english_sequences,english_word2idx,english_length,english_idx2word = en_preprocessor.Tokenizer_text(english_text_input,VOCAB_SIZE) \n",
        "english_sequences = en_preprocessor.padding()\n",
        "english_totalwords = len(english_word2idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q2PfZHVUKuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing for French Text\n",
        "\n",
        "fra_preprocessor = Preprocessing(VOCAB_SIZE=VOCAB_SIZE) \n",
        "french_sequences,french_word2idx,french_length,french_idx2word = fra_preprocessor.Tokenizer_text(french_text_input,VOCAB_SIZE) \n",
        "french_sequences = fra_preprocessor.padding()\n",
        "french_totalwords = len(french_word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJYY55KDg7no",
        "colab_type": "text"
      },
      "source": [
        "# DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Jtd0_6g44C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TranslatorDataset creates a dataset for easy batch creation\n",
        "\"\"\"\n",
        "class TranslatorDataset(Dataset):\n",
        "  \n",
        "  \"\"\"\n",
        "  Args\n",
        "  english_sequences is the source language sequences\n",
        "  french_sequences is the target language sequences\n",
        "  \"\"\"\n",
        "  def __init__(self,english_sequences,french_sequences):\n",
        "    self.english_sequences = english_sequences\n",
        "    self.french_sequences = french_sequences\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.english_sequences)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    english = self.english_sequences[index]\n",
        "    french = self.french_sequences[index]\n",
        "    \n",
        "    return english,french"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1TAk8kDhB9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = TranslatorDataset(english_sequences,french_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yn6uJPpg5eq",
        "colab_type": "text"
      },
      "source": [
        "# MODEL CREATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hijuNvqMU1Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Transformer(embed_dim=EMBEDDING_DIM,heads=8,number=6,max_seq_length_source=english_length,max_seq_length_target=french_length,source_vocab=english_totalwords+1,target_vocab=french_totalwords+1)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "        \n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBK82BMjZWTz",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkazoQYgZQ7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs,print_timestep = 100):\n",
        "  \n",
        "  model.train()\n",
        "  \n",
        "  start = time.time()\n",
        "  temp = start\n",
        "  \n",
        "  total_loss = 0\n",
        "  \n",
        "  # Creating the Dataloader for the dataset\n",
        "  \n",
        "  dataloader = DataLoader(data, batch_size=BATCH_SIZE,shuffle=False)\n",
        "  \n",
        "  # Appliyng the masking\n",
        "  source_masking = Masking(sequencetype='Source',paddingvalue=0)\n",
        "  target_masking = Masking(sequencetype='Target',paddingvalue=0)\n",
        "  \n",
        "  # Rrunning epochs\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # Extracting batches\n",
        "    for i_batch, sample_batched in enumerate(dataloader):\n",
        "      \n",
        "      # Extracting the source and target and converting into tensor\n",
        "      \n",
        "      source = torch.tensor(sample_batched[0]).to(torch.int64)\n",
        "      target = torch.tensor(sample_batched[1]).to(torch.int64)\n",
        "      target_input = target[:,:-1]\n",
        "      target_output = target[:,1:].contiguous().view(-1)\n",
        "      \n",
        "      #masking\n",
        "      source_mask = source_masking.create_mask(source)\n",
        "      target_mask = target_masking.create_mask(target_input)\n",
        "      \n",
        "#       print('Source',source.size())\n",
        "#       print('Source',target_input.size())\n",
        "#       print('Source',source_mask.size())\n",
        "#       print('Source',target_mask.size())\n",
        "\n",
        "      # Predicting\n",
        "      preds = model(source,target_input,source_mask,target_mask)\n",
        "      optim.zero_grad()\n",
        "\n",
        "      # Calculating Loss\n",
        "      loss = F.cross_entropy(preds.view(-1, preds.size(-1)),target_output, ignore_index=0)\n",
        "      \n",
        "      # Updating weights\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      \n",
        "      total_loss += loss.item()\n",
        "      if (i_batch + 1) % print_timestep == 0:\n",
        "        loss_avg = total_loss / print_timestep\n",
        "        print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60,epoch + 1, i_batch + 1, loss_avg,time.time() - temp,print_timestep))\n",
        "        total_loss = 0\n",
        "        temp = time.time()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96nqcSOUgw3r",
        "colab_type": "code",
        "outputId": "5c240ba1-f994-443f-dcb9-745c302c56b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train(1)\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time = 2m, epoch 1, iter = 100, loss = 6.446, 136s per 100 iters\n",
            "time = 4m, epoch 1, iter = 200, loss = 5.617, 135s per 100 iters\n",
            "time = 6m, epoch 1, iter = 300, loss = 5.509, 132s per 100 iters\n",
            "time = 8m, epoch 1, iter = 400, loss = 5.518, 133s per 100 iters\n",
            "time = 11m, epoch 1, iter = 500, loss = 5.489, 132s per 100 iters\n",
            "time = 13m, epoch 1, iter = 600, loss = 5.446, 131s per 100 iters\n",
            "time = 15m, epoch 1, iter = 700, loss = 5.388, 131s per 100 iters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqbYOsMisLZN",
        "colab_type": "text"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqCC7qOtDpE",
        "colab_type": "code",
        "outputId": "e4e30ce2-0dac-4cc5-b67c-9d6d1f6f0767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Generating a random input\n",
        "i = np.random.choice(len(english_text_input[:25000]))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Preprocessing taks\n",
        "input_sequence = english_sequences[i:i+1]\n",
        "input_sequence = torch.from_numpy(input_sequence).to(torch.int64)\n",
        "\n",
        "source_masking = Masking(sequencetype='Source',paddingvalue=0)\n",
        "target_masking = Masking(sequencetype='Target',paddingvalue=0)\n",
        "\n",
        "source_mask = source_masking.create_mask(input_sequence)\n",
        "\n",
        "# Getting the encoder output to pass to the input\n",
        "encoder_output = model.Encoder(input_sequence,source_mask)\n",
        "\n",
        "# creating an output tensor array with starting as <sos>\n",
        "\n",
        "outputs = torch.zeros(13).type_as(input_sequence.data)\n",
        "outputs[0] = torch.LongTensor([french_word2idx['<sos>']])\n",
        "\n",
        "# Running for the maximum seq length\n",
        "for j in range(1,french_length-1):\n",
        "  \n",
        "  # Target Mask\n",
        "  trg_mask = np.triu(np.ones((1, j, j)),k=1).astype('uint8')\n",
        "  trg_mask= Variable(torch.from_numpy(trg_mask) == 0)\n",
        "  \n",
        "  # Calculating output\n",
        "  out = model.outputlayer(model.Decoder(outputs[:j].unsqueeze(0),encoder_output, source_mask, trg_mask))  # unsqueezing it to match the expected size\n",
        "  out = F.softmax(out, dim=-1)\n",
        "  \n",
        "#  print(out[:,-1].data.topk(1))\n",
        "  # Extracting index with maximum probability\n",
        "  \n",
        "  val, ix = out[:, -1].data.topk(1)\n",
        "  \n",
        "  outputs[j] = ix[0][0]\n",
        "  \n",
        "  if ix[0][0] == french_word2idx['<eos>']:\n",
        "    break\n",
        "  \n",
        "  \n",
        "\n",
        "print('English: ',english_text_input[i])\n",
        "print('French: ',french_text_input[i])\n",
        "print('French Predicted: ',\" \".join(french_idx2word[int(index.numpy())] for index in outputs[1:j]))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English:  I'm calm.\n",
            "French:  <sos> Je suis calme. <eos>\n",
            "French Predicted:  je suis ne ne ne ne ne ne ne ne ne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_CrRmLTkNUF",
        "colab_type": "text"
      },
      "source": [
        "#  SAVING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIMRWYW-w5B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/Dataset/model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCjETGF_kPfj",
        "colab_type": "text"
      },
      "source": [
        "# LOADING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOWphVgah-Gb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab604319-9751-4734-ab25-54da87452f5d"
      },
      "source": [
        "model2 = Transformer(embed_dim=EMBEDDING_DIM,heads=8,number=6,max_seq_length_source=english_length,max_seq_length_target=french_length,source_vocab=english_totalwords+1,target_vocab=french_totalwords+1)\n",
        "model2.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/Dataset/model.pth'))\n",
        "model2.eval()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (Encoder): Encoder(\n",
              "    (embedded_layer): Embedder(\n",
              "      (embed): Embedding(9494, 512)\n",
              "    )\n",
              "    (PositionEmbedding): PositionEmbedding()\n",
              "    (encoderlayers): ModuleList(\n",
              "      (0): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "      (1): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "      (2): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "      (3): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "      (4): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "      (5): EncoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (mhattention): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (normalization): Normalization()\n",
              "  )\n",
              "  (Decoder): Decoder(\n",
              "    (embedded_layer): Embedder(\n",
              "      (embed): Embedding(17972, 512)\n",
              "    )\n",
              "    (PositionEmbedding): PositionEmbedding()\n",
              "    (decoderlayers): ModuleList(\n",
              "      (0): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "      (1): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "      (2): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "      (3): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "      (4): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "      (5): DecoderStructure(\n",
              "        (normalize_1): Normalization()\n",
              "        (attention_1): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1)\n",
              "        (normalize_2): Normalization()\n",
              "        (attention_2): MultiHeadedAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_2): Dropout(p=0.1)\n",
              "        (normalize_3): Normalization()\n",
              "        (feedforward): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_3): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (normalization): Normalization()\n",
              "  )\n",
              "  (outputlayer): Linear(in_features=512, out_features=17972, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yivmAFprzyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}